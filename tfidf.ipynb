{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleaning: TF-IDF\n",
    "\n",
    "## 0. Overview:\n",
    "\n",
    "Conside the two one-sentence documents below:\n",
    "\n",
    "1. This is the first sentence.\n",
    "2. Here is another sentence.\n",
    "\n",
    "While the written language is straighforward for a human reader to evaluate, an automated system will need to translate the written word into a more machine-friendly format that will allow us to use data science techniques to grade the essays. A simple method, would be to simply count up the occurences of each word, called a counting vectorizer. In that case, the two documents above would become:\n",
    "\n",
    "| |another|first|here|is|sentence|the|this|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|**This is the first sentence.**|0.000|1.000|0.000|1.000|1.000|1.000|1.000|\n",
    "|**Here is another sentence.**|1.000|0.000|1.000|1.000|1.000|0.000|0.000|\n",
    "\n",
    "However, the algorithm we have chosen is called [term frequency-inverse document frequency](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), or **tf-idf** (\"term frequency\" because it emphasizes words that appear often in a particular essay, and \"inverse document frequency\" because it penalizes words that appear often in all documents). Essentially, this method takes as an input a set of text (in our case an essay) and *vectorizes* it, outputting a vector of numbers corresponding to that document.\n",
    "\n",
    "To create this output vector, a **tf-idf** algorithm counts the number of times a word appears in a document and scales it according to the formula\n",
    "\n",
    "$$\n",
    "w_{j,i} = tf_{j,i} \\left(\\log\\frac{1 + N}{1 + df_i} + 1\\right),\n",
    "$$\n",
    "\n",
    "where $tf_{i,j}$ is the number of times term $i$ occurs in essay $j$, $N$ is the total number of essays, $df_i$ is the number of documents containing term $i$, and $w_{j,i}$ is the tf-idf weighted term vector for this sentence. The particular tf-idf vectorizer we chose also normalizes each document's tf-idf vector by its 2-norm to make it a unit vector (that is, $\\hat{w}_{j,:} = w_{j,:} / ||w_{j,:}||_2$). The same two sentences run through tf-idf vectorizer look like:\n",
    "\n",
    "| |another|first|here|is|sentence|the|this|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|**This is the first sentence.**|0.000|0.499|0.000|0.355|0.355|0.499|0.499|\n",
    "|**Here is another sentence.**|0.576|0.000|0.576|0.410|0.410|0.000|0.000|\n",
    "\n",
    "Notably, the words \"sentence\" and \"is\" count less in both documents, since they're repeated in both. Creating these vectors for every essay and stacking them on top of one another provides **tf-idf matrix**, a straightforward method for letting a computer handle text. Conveniently, taking any column from this matrix gives the (weighted) number of times a given word appears in each essay. Taking every column into account, we can now build a model that scores an essay based on the *word-content* of that essay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| |another|first|here|is|sentence|the|this|\n",
      "|---|---|---|---|---|---|---|---|\n",
      "|**This is the first sentence.**|0.000|1.000|0.000|1.000|1.000|1.000|1.000|\n",
      "|**Here is another sentence.**|1.000|0.000|1.000|1.000|1.000|0.000|0.000|\n",
      "\n",
      "\n",
      "| |another|first|here|is|sentence|the|this|\n",
      "|---|---|---|---|---|---|---|---|\n",
      "|**This is the first sentence.**|0.000|0.499|0.000|0.355|0.355|0.499|0.499|\n",
      "|**Here is another sentence.**|0.576|0.000|0.576|0.410|0.410|0.000|0.000|\n"
     ]
    }
   ],
   "source": [
    "# The below code calculates the examples shown above.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def toMarkDownTable(vect, sentences):\n",
    "    # Export a markdown table of the vectorizer output\n",
    "    \n",
    "    # Fit\n",
    "    wordVec = vect.fit_transform(sentences)\n",
    "    \n",
    "    vectWords = ['|', ' ']\n",
    "    vectCounts1 = ['|', '**', sentences[0], '**']\n",
    "    vectCounts2 = ['|', '**',sentences[1], '**']\n",
    "    dividers = ['|', '---']\n",
    "\n",
    "    # for word in vect.vocabulary_:\n",
    "    for word in vect.get_feature_names():\n",
    "        vectWords += '|%s'%word\n",
    "        dividers += '|---'\n",
    "    vectWords += '|'\n",
    "    dividers += '|'\n",
    "\n",
    "    for count in np.array(wordVec.todense()[0,:]).reshape(-1):\n",
    "        vectCounts1 += '|%4.3f'%count\n",
    "    vectCounts1 += '|'\n",
    "\n",
    "    for count in np.array(wordVec.todense()[1,:]).reshape(-1):\n",
    "        vectCounts2 += '|%4.3f'%count\n",
    "    vectCounts2 += '|'\n",
    "\n",
    "    # Print out the results\n",
    "    print ''.join(vectWords)\n",
    "    print ''.join(dividers)\n",
    "    print ''.join(vectCounts1)\n",
    "    print ''.join(vectCounts2)\n",
    "    \n",
    "\n",
    "# The example sentences\n",
    "exampleSentence = 'This is the first sentence.'\n",
    "sentenceTwo = \"Here is another sentence.\"\n",
    "\n",
    "# Print out the tables\n",
    "toMarkDownTable(CountVectorizer(), [exampleSentence, sentenceTwo])\n",
    "print '\\n'\n",
    "toMarkDownTable(TfidfVectorizer(norm='l2'), [exampleSentence, sentenceTwo])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
