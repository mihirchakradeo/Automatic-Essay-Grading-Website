{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After vectorizing our essays with `sklearn`'s tf-idf vectorizer, we have a sparse matrix of vectorized text, often with *more than 16,000* different features. Outsized feature sets like this are ripe for overfitting, as shown in our baseline model, so we chose to perform **dimensionality reduction** to reduce the total number of features going into our model. A powerful tool at our disposal was **latent semantic analysis**, (LSA). Essentially, LSA takes in the large feature set of individual words and finds combinations of those words that explain most of the variability in the original text. This means that after performing LSA on a tf-idf matrix with thousands of features, we can use a much-reduced model with substantively fewer predictors in our model, avoiding some of the risk of overfitting.\n",
    "\n",
    "To explain how LSA works, consider the two one-sentence documents below:\n",
    "1. \"Here is one document.\"\n",
    "2. \"Here is another.\"\n",
    "\n",
    "As before, we can vectorize these two documents using a tf-idf algorithm. For the sake of explanation here, we will demonstrate LSA using a counting vectorizer, which simply counts the occurences of each term, but LSA part of the analysis is exactly the same no matter how we vectorize the text. As shown in the below code, after vectorization, the two documents become:\n",
    "\n",
    "| |another|document|here|is|one|this|\n",
    "|---|---|---|---|---|---|---|\n",
    "|**Here is one document.** |0|1|1|1|1|0|\n",
    "|**Here is one document.** |1|0|0|1|0|1|\n",
    "\n",
    "A single LSA component will give each term in this tf-idf matrix a weight, multiply a document's tf-idf (row) vector by those weights, then sum across all the different terms. Effectively, it is transforming the high-dimensional tf-idf matrix into a much lower-dimensional space. To make these weights as useful as possible, LSA orders these weights such that the first LSA component explains the most variance in original feature sets, with subsequent components explaining progressively less variance. LSA components must also be orthogonal (forming right angles with each other, so they are independent) and of unit magnitude (so they don't change the original feature set's shape). It is possible to create as many LSA components as there are original features, but doing so would defeat the purpose of performing LSA. As show below, `sklearn` can implement LSA via `Truncated SVD`. In the simple case of one LSA component, the calculation looks like this:\n",
    "\n",
    "|  |another|document|here|is|one|this| **Sum**|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|**LSA weight: ** |0.24|0.4|0.4|0.64|0.4|0.24|--|\n",
    "|**Weighted doc. 1** |0.0|0.4|0.4|0.64|0.4|0.0| **1.83**|\n",
    "|**Weighted doc. 2** |0.24|0.0|0.0|0.64|0.0|0.24| **1.13**|\n",
    "\n",
    "Recall that while this example uses a counting vectorizer, our analysis used a tf-idf vectorizer. Our 2 x 6 vectorized set of documents has become a 2 x 1 matrix with much lower dimensionality. \n",
    "\n",
    "Abstracting this concept to the vastly higher-dimensional tf-idf is straightforward, but it leaves the choice of how many LSA components to use in the final model, a hyperparameter we call $d$. Again, the above example uses $d = 1$, but it is possible to choose $d$ up to the original number of features. In our case, we used cross validation to tune $d$, by varying it, training a new model with $d$ components from a training set, and evaluating that model's %R^2$ on a testing set. The results for each essay set are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tuning d](figures/d_tuning.png \"Tuning d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  |another|document|here|is|one|this|\n",
      "|---|---|---|---|---|---|---|\n",
      "|**Here is one document.** |0|1|1|1|1|0|\n",
      "|**Here is one document.** |1|0|0|1|0|1|\n",
      "\n",
      "\n",
      "[ 0.09750776  0.90249224]\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def toTableRow(array):\n",
    "    # Convert an array to a markdown table row\n",
    "    \n",
    "    row = []\n",
    "    for element in array:\n",
    "        row += '|%s'%str(element)\n",
    "    \n",
    "    row += '|'\n",
    "    \n",
    "    return ''.join(row)\n",
    "        \n",
    "\n",
    "sentences = ['Here is one document.',\n",
    "             'This is another.']\n",
    "\n",
    "# Vectorize the text\n",
    "vectorizer = CountVectorizer()\n",
    "word_mat = np.array(vectorizer.fit_transform(sentences).todense())\n",
    "\n",
    "\n",
    "# Perform LSA\n",
    "n_components = 5\n",
    "lsa_model = TruncatedSVD(n_components=n_components)\n",
    "lsa_transformed = lsa_model.fit_transform(word_mat)\n",
    "\n",
    "# Print out a markdown table of the vectorized text\n",
    "n_features = len(vectorizer.get_feature_names())\n",
    "\n",
    "print '| ', toTableRow(vectorizer.get_feature_names())\n",
    "print toTableRow(np.tile('---', (n_features + 1,)))\n",
    "print '|**%s**'%sentences[0], toTableRow(word_mat[0,:])\n",
    "print '|**%s**'%sentences[0], toTableRow(word_mat[1,:])\n",
    "\n",
    "print '\\n'\n",
    "\n",
    "\n",
    "# Print out a markdown table of the LSA calculation\n",
    "# print '| ', toTableRow(vectorizer.get_feature_names()), '**Sum**|'\n",
    "# print toTableRow(np.tile('---', (n_features + 2,)))\n",
    "# print '|**LSA weight: **', toTableRow(np.round(lsa_model.components_[0,:], 2)), ' |'\n",
    "# print '|**Weighted doc. 1**', toTableRow(np.round(\n",
    "#                                             np.multiply(lsa_model.components_[0,:], \n",
    "#                                             word_mat[0,:]),\n",
    "#                                             2)), '**%.2f**|'%round(lsa_transformed[0], 2)\n",
    "\n",
    "# print '|**Weighted doc. 2**', toTableRow(np.round(\n",
    "#                                             np.multiply(lsa_model.components_[0,:], \n",
    "#                                             word_mat[1,:]),\n",
    "#                                             2)), '**%.2f**|'%round(lsa_transformed[1], 2)\n",
    "\n",
    "print lsa_model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the two documents produced six features. LSA will weight each of these features according to the formula\n",
    "\n",
    "$$\n",
    "v^{LSA}_{i, d} = \\phi_{d,1} \\cdot tf_{1} + \\cdots + \\phi_{d,j} \\cdot tf_{j} + \\cdots + \\phi_{d,n} \\cdot tf_{n}\n",
    "$$\n",
    "\n",
    "where $v^{LSA}_{i,d}$ is the $i^{th}$ element of the $d^{th}$ LSA component, $[tf_1, \\cdots tf_n]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
