{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Meta-features\n",
    "\n",
    "Of course, a good essay is more than the sum of its words. Even weighted by the tf-idf algorithm, using just word frequencies will miss many of the subtleties that separate good writing from bad. To capture some of these components, we also analyzed a number of *meta-features*, or features that look at the essay as a whole, rather than just the contents. In no particular order, the meta-features we analyzed were:\n",
    "\n",
    "### **Cosine similarity to the essay prompt**\n",
    "Vectorization is particularly useful in automatic essay grading because it allows us to translate more conventional math principles to the written word. One of the more relevant math principles, cosine similarity, allows us to quantify how similar a given essay is to its prompt. In three-dimensional space, the formula\n",
    "\n",
    "$$\n",
    "\\mathrm{similarity} = \\cos\\theta = \\frac{\\vec{a} \\bullet \\vec{b}}{||a|| \\cdot ||b||}\n",
    "$$\n",
    "\n",
    "calculates the smallest included angle between vectors $\\vec{a}$ and $\\vec{b}$. If $\\vec{a}$ and $\\vec{b}$ are colinear (on the same line in space), $\\cos\\theta = 1$, and if $\\vec{a}$ and $\\vec{b}$ form a right angle, $\\cos\\theta = 0$. If $\\vec{a}$ and $\\vec{b}$ point in exactly opposite directions $\\cos\\theta = -1$. Abstracting this principle to the tf-idf vectors of an essay and its prompt demonstrates its value in automatic essay assessment: essays very close to the prompt will receive similarity scores close to 1, while completely unrelated essays will receive lower scores. As an example, consider the following three sentences:\n",
    ">1. This is the first sentence.\n",
    "\n",
    ">2. This intelligent second sentence is related to the first sentence.\n",
    "\n",
    ">3. Completely unrelated nonsense scores zero.\n",
    "\n",
    "The cosine similarities bewteen these three sentences are:\n",
    "\n",
    "**\"This is the first sentence.\"** vs. **\"This intelligent second sentence is related to the first sentence.\"**\n",
    "\n",
    "$$\\mathrm{similarity} = 0.69$$\n",
    "\n",
    "**\"This is the first sentence.\"** vs. **\"Completely unrelated nonsense scores zero.\"**\n",
    "\n",
    "$$\\mathrm{similarity} = 0.00$$\n",
    "\n",
    "**\"This is the first sentence.\"** vs. **\"This is the first sentence.\"**\n",
    "\n",
    "$$\\mathrm{similarity} = 1.00$$\n",
    "\n",
    "\n",
    "\n",
    "While the benefits of this analysis are clear, so are its drawbacks. A writer who simply repeats the prompt would receive a perfect score in cosine similarity, a nonsensical conclusion that runs completely contrary to the way a human grader would assess an essay. Therefore, it must be just one of many meta-features used in our model.\n",
    "\n",
    "### **Essay length in words**\n",
    "\n",
    "In general, a longer essay allows the writer to more completely answer the prompt. Like cosine similarity, however, this meta-feature is insufficient as the sole metric for grading essays. A long, rambling response would score better than more succinct replies, again running counter to the behavior of a human grader.\n",
    "\n",
    "\n",
    "### **Mean word length, mean sentence length, and number of unique words**\n",
    "All related, these measures inform the model about the overall complexity of a particular essay. Sophisticated writers are likely to use longer words, longer sentences, and more words in general. However, these meta-features come with the same caveats as above: taken independently, they fail to accurately reproduce the work of a human grader.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
